{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo_NC_Scores.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tqdm import tqdm\n",
        "from LEEP import LEEP\n",
        "from LogME import LogME\n"
      ],
      "metadata": {
        "id": "MPyG3yXmeASv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import np_utils\n",
        "import cv2\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "\n",
        "def load_cifar10_data(img_rows, img_cols):\n",
        "\n",
        "    # Load cifar10 training and validation sets\n",
        "    (X_train, Y_train), (X_valid, Y_valid) = cifar10.load_data()\n",
        "    nb_train_samples = 40000 \n",
        "    nb_valid_samples = 10000 \n",
        "    num_classes = 10\n",
        "\n",
        "    # Resize trainging images\n",
        "\n",
        "    X_train = np.array([cv2.resize(img, (img_rows,img_cols)) for img in X_train[:nb_train_samples,:,:,:]])\n",
        "    X_valid = np.array([cv2.resize(img, (img_rows,img_cols)) for img in X_valid[:nb_valid_samples,:,:,:]])\n",
        "\n",
        "    # Transform targets to keras compatible format\n",
        "    Y_train = np_utils.to_categorical(Y_train[:nb_train_samples], num_classes)\n",
        "    Y_valid = np_utils.to_categorical(Y_valid[:nb_valid_samples], num_classes)\n",
        "\n",
        "    return X_train, Y_train, X_valid, Y_valid"
      ],
      "metadata": {
        "id": "so7HCbC4SWpH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train, X_valid, Y_valid=load_cifar10_data(32, 32)"
      ],
      "metadata": {
        "id": "JFcc5Zob-EGr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "tf.keras.backend.set_floatx('float64')  \n",
        "\n",
        "class Model1(tf.keras.Model):\n",
        "    def __init__(self,feature_layer,**kwargs):\n",
        "        super().__init__()\n",
        "        self.d1=feature_layer\n",
        "        # self.d2=layers.Dropout(0.25)\n",
        "        self.d5=layers.Dense(10,activation='softmax')\n",
        "\n",
        "    def forward(self, x,train=False):\n",
        "        x = self.d1(x)[:,0,0,:]\n",
        "        # x=self.d2(x)\n",
        "        x = self.d5(x)        \n",
        "        return x\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.forward(inputs,train=False)\n",
        "        return x     \n",
        "\n",
        "\n",
        "\n",
        "# from tensorflow.keras.applications.resnet import ResNet50\n",
        "# base_model = ResNet50(weights='imagenet', include_top=True,input_tensor=keras.layers.Input(shape=(32, 32, 3))) ### probablistic predictions, required for LEEP\n",
        "\n",
        "# from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
        "# base_model = EfficientNetB0(weights='imagenet', include_top=True,input_tensor=keras.layers.Input(shape=(32, 32, 3))) ### probablistic predictions, required for LEEP\n",
        "\n",
        "# from tensorflow.keras.applications.xception import Xception\n",
        "# base_model = Xception(weights='imagenet', include_top=True,input_tensor=keras.layers.Input(shape=(32, 32, 3))) ### probablistic predictions, required for LEEP\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "# base_model = MobileNetV2(weights='imagenet', include_top=True,input_tensor=keras.layers.Input(shape=(32, 32, 3))) ### probablistic predictions, required for LEEP\n",
        "\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "base_model = DenseNet121(weights='imagenet', include_top=True,input_tensor=keras.layers.Input(shape=(32, 32, 3))) ### probablistic predictions, required for LEEP\n",
        "\n",
        "\n",
        "base_model_emb = tf.keras.Model(inputs=base_model.inputs, outputs=base_model.layers[-2].output)  ### Gives embedding"
      ],
      "metadata": {
        "id": "tV2MgvuESdvx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83a9213b-2cf7-4b53-db52-b4d21f3808a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels.h5\n",
            "33193984/33188688 [==============================] - 1s 0us/step\n",
            "33202176/33188688 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_neural_collapse_score(E,L):\n",
        "   Ind=np.unique(L)\n",
        "   class_means=[]\n",
        "   \n",
        "   for i in Ind:  \n",
        "     ind=np.where(L==i)[0]\n",
        "     data=E[ind,:]\n",
        "     mean=np.mean(data,axis=0)\n",
        "     class_means.append(mean)\n",
        "     \n",
        "   num=0 \n",
        "   for i in range(0,len(E)):\n",
        "       a=np.linalg.norm(E[i]-class_means[int(L[i])]) \n",
        "       num=num+a\n",
        "   N=num/len(E)\n",
        "\n",
        "\n",
        "   num=0 \n",
        "   sum=0\n",
        "   for i in range(0,len(class_means)):\n",
        "       c1=class_means[i]\n",
        "       for j in range(0,len(class_means)):\n",
        "           c2=class_means[j]\n",
        "           if i!=j:\n",
        "              sum=sum+np.linalg.norm(c1-c2) \n",
        "              num=num+1\n",
        "   D=sum/num\n",
        "   score=N/D\n",
        "    \n",
        "   return score"
      ],
      "metadata": {
        "id": "o-xyg845xQLN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(X_train,Y_train,model): \n",
        "    X=np.array_split(X_train,50)\n",
        "    Y=np.array_split(Y_train,50)\n",
        "    E=[]\n",
        "    L=[]\n",
        "\n",
        "    for i in tqdm(range(len(X))):\n",
        "        e=model(X[i])\n",
        "        E.append(e)\n",
        "        L.append(Y[i])\n",
        "     \n",
        "    E=np.vstack(E)\n",
        "    L=np.vstack(L)\n",
        "    return E, L\n"
      ],
      "metadata": {
        "id": "4v6knSAIazMU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb, label= get_embedding(X_train,Y_train,base_model_emb)\n",
        "L1=np.argmax(label,axis=1)\n",
        "nc = compute_neural_collapse_score(emb,L1)\n",
        "print(nc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPLZ2oKbcEij",
        "outputId": "4dc3e82f-5630-4737-c103-bf5e041726d3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [01:25<00:00,  1.72s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.273530291811587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logme = LogME(regression=False)\n",
        "score=logme.fit(emb,L1)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klgTT80Rbp0T",
        "outputId": "a95e1aa0-e011-4de0-9cb6-b4104130187e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.07839117674345226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds, label= get_embedding(X_train,Y_train,base_model)\n",
        "L1=np.argmax(label,axis=1)\n",
        "leep = LEEP(preds,L1)\n",
        "print(leep)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUXFsOalbuo-",
        "outputId": "e4dccf16-f825-4b10-a3ab-b91901d1b3b3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [01:22<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-2.078625739711616\n"
          ]
        }
      ]
    }
  ]
}